---
title: "R/Business - Insurance Fraud Detection"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,   
                      message = FALSE,
                      warning = FALSE)
```

## R Business - Insurance Fraud Detection I

Our goal at R Business is to encourage the use of R Studio in a varity of industries where it is current use has been underdevloped. One of these industries is insurance where Excel, VBA and SAS still seem to be the dominant forces. This tutorial is meant to show a simple example of what R Studio is capable of in an insurance related Use Case. Namely the detection of potential fraudulent activity. This is an extremely simplified data set and workflow, but the basic procedure remains the same even in more complex real world examples.

## Assumptions

This tutorial assumes some basic competency with [R Studio](https://rstudio.com/) and the [Random Forest](https://en.wikipedia.org/wiki/Random_forest) method. If you aren't familiar with these I would recommend you first read up on them in the links provided. 

## Load Libraries

The first step is to load all the libraries you will need during this example. We will need quite a lot of them in order to properly run this analysis. One of the main benefits of R Studio is the richness of high quality libraries for complex statistical methods.

```{r lib, warning=FALSE}
# Load Libraries
library(dplyr)
library(readr)
library(randomForest)
library(rfUtilities)
library(rsample)
library(ggplot2)
library(caret)
library(e1071)
library(skimr)
```

## Load Data

The next step is to load the data and perform an initial check. This particular dataset if from [Kaggle](https://www.kaggle.com/buntyshah/auto-insurance-claims-data). I would recommend downloading it and putting it into your [local directory](https://support.rstudio.com/hc/en-us/articles/200711843-Working-Directories-and-Workspaces).

```{r data}
#Loading and Discovery

# Load Data
data <- read_csv("insurance_claims.csv")

# First Look at Data
head(data)
```

## Inspect Data

Since everything seems fine we should dive a little deeper into it. Let's check data types and NAs. The "skim" function also provides a lot of insight.

```{r inspect}
# Check Types
sapply(data, class)

# Check for NAs 
data[!complete.cases(data),] #non found

# Summarize Data
skim(data)
```

## Data Wrangling - Prepare Data for Random Forest

This part is where it gets (a little) tricky. If we had found any NAs we would have had to remove them at this point. The na.omit() function makes that very easy. Good data cleaning can be about 80% of the work in these projects, but luckliy we already have some very clean data. All we need to do here is convert some of the column types to more sensical formats.

```{r wrangle, echo=FALSE}
#Data Wrangling
data$insured_zip <- as.character(data$insured_zip)
data$policy_bind_date <- as.Date(data$policy_bind_date, format = '%m/%d/%y')
```

# Prepare Data for Random Forest

Next we turn the character columns into factors, so that they work with the Random Forest function. We also check the amount of unique values per column.

```{r prep}
# turn into factors
data <- data %>% mutate_if(is.character, as.factor)

# Check for amount of uniqe values
sapply(sapply(data, unique), length)
```

Random Forest wont work if there are too many unique values in a factor, which is why we remove those above a certain threshhold. This is more art than science though. The Random Forest will give you a warning if you exceed certain limits.

```{r prep 2}
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !(names(data) %in% drops)]
```

## Split Data

As in any good analysis we need test and training data. We use a seed so that others ca replicate the random process later.

```{r split}

# For reproducability
set.seed(52)

# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
```

## Basic Implementation & Analysis

Now we get to the really fun part. R makes it easy to implement a Random Forest. This will act as our baseline for tuning.

```{r rf}
# Basic Implementation & Analysis

# default RF model
rf1 <- randomForest(
  formula = fraud_reported ~ .,
  data    = data_train)

# Model
rf1
```

# Check Results

This doesn't look bad for a first try. Let's get a better look by visualizing the performance.

```{r check}
# Plot tree
plot(rf1)

#Variable Importance
rf1$importance
```
## RF Tuning

In order to improve our initial results we can use the tuneRF() function. It will help us find the right values for the number of trees.

```{r tune}
# create training and validation data 
data_split <- initial_split(data_train, .8)

# validation data
data_valid <- assessment(data_split)

# names of features
features <- setdiff(names(data_valid),"fraud_reported")

# tune Random Forest
rf2 <- tuneRF(
  x          = data_train[features],
  y          = data_train$fraud_reported,
  ntreeTry   = 500,
  mtryStart  = 5,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = TRUE      # to not show real-time progress 
)
```


# Check results

Nice Improvement!

```{r rf2}
# Try best version
rf2 <- randomForest(
  x          = data_train[features],
  y          = data_train$fraud_reported,
  data    = data_train,
  ntree = 500,
  mtry = 22
)

#Check results
rf2
```

# Vizualize 

```{r check2}
# Plot tree
plot(rf2)

#Variable Importance
rf2$importance
```

## Predicting

Now we use this RF model to predict the test set. This seems to still be a bit buggy now.

```{r pred}
# randomForest
data_test$pred <- predict(rf2, data_test)
table(observed = data_test$fraud_reported, predicted = data_test$pred)
```

