plot(rf2)
#Variable Importance
# rf2$importance
# randomForest
pred_randomForest <- predict(rf2, data_test)
head(pred_randomForest)
# Make pred column
data_test$pred <- predict(rf2, data_test)
data_test$pred_final <- as.integer(ifelse(data_test$pred > 0.3 ,1 ,0))
# Confusion Matrix
confusionMatrix(as.factor(data_test$fraud_reported), as.factor(data_test$pred_final))
knitr::opts_chunk$set(echo = TRUE,
message = FALSE,
warning = FALSE)
# Load Libraries
library(dplyr)
library(readr)
library(randomForest)
library(rfUtilities)
library(rsample)
library(ggplot2)
library(caret)
library(e1071)
install.packages("skimr")
knitr::opts_chunk$set(echo = TRUE,
message = FALSE,
warning = FALSE)
# Load Libraries
library(dplyr)
library(readr)
library(randomForest)
library(rfUtilities)
library(rsample)
library(ggplot2)
library(caret)
library(e1071)
library(skimr)
#Loading and Discovery
# Load Data
data <- read_csv("insurance_claims.csv")
# First Look at Data
head(data)
# Check Types
sapply(data, class)
# Check for NAs
data[!complete.cases(data),] #non found
# Summarize Data
skim(data)
#Data Wrangling
#data$fraud_reported[data$fraud_reported == 'Y']  <- 1
#data$fraud_reported[data$fraud_reported == 'N']  <- 0
data$fraud_reported <- as.integer(data$fraud_reported)
data$insured_zip <- as.character(data$insured_zip)
data$policy_bind_date <- as.Date(data$policy_bind_date, format = '%m/%d/%y')
# Check for amount of uniqe values
sapply(sapply(data, unique), length)
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , -drops]
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !drops]
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , -drops]
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !(names(data) %in% drops)]
# turn into factors
data <- data %>% mutate_if(is.character, as.factor)
#data$fraud_reported <- as.factor(data$fraud_reported)
# For reproducability
set.seed(52)
# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
# For reproducability
set.seed(52)
# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
#Data Wrangling
#data$fraud_reported[data$fraud_reported == 'Y']  <- 1
#data$fraud_reported[data$fraud_reported == 'N']  <- 0
data$fraud_reported <- as.integer(data$fraud_reported)
data$insured_zip <- as.character(data$insured_zip)
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !(names(data) %in% drops)]
# turn into factors
data <- data %>% mutate_if(is.character, as.factor)
#data$fraud_reported <- as.factor(data$fraud_reported)
# For reproducability
set.seed(52)
# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
# Basic Implementation & Analysis
# default RF model
rf1 <- randomForest(
formula = fraud_reported ~ .,
data    = data_train)
install.packages("censusapi")
# Load Libraries
library(censusapi)
# Add key to .Renviron
Sys.setenv(CENSUS_KEY= d4d19011ae2f65d497438119997a3d39b2945356)
# Reload .Renviron
readRenviron("~/.Renviron")
# Add key to .Renviron
Sys.setenv(CENSUS_KEY= 'd4d19011ae2f65d497438119997a3d39b2945356')
# Reload .Renviron
readRenviron("~/.Renviron")
usethis::edit_r_environ()
# Reload .Renviron
readRenviron("~/.Renviron")
# Check to see that the expected key is output in your R console
Sys.getenv("CENSUS_KEY")
# Test it out
getCensus(name = "timeseries/healthins/sahie",
vars = c("NAME", "IPRCAT", "IPR_DESC", "PCTUI_PT"),
region = "state:01",
time = 2017)
sahie_counties <- getCensus(name = "timeseries/healthins/sahie",
vars = c("NAME", "IPRCAT", "IPR_DESC", "PCTUI_PT"),
region = "county:*",
regionin = "state:01",
time = 2017)
head(sahie_counties, n=12L)
df <- getCensus(name = "acs/acs5", vintage = 2017,
vars = c("B01001_001E", "NAME", "B01002_001E", "B19013_001E"),
region = "tract:*", regionin = "state:06")
head(df)
# Use American Community Survey variable groups to get all data from a given table.
# This returns estimates as well as margins of error and annotation flags.
acs_group <- getCensus(name = "acs/acs5",
vintage = 2017,
vars = c("NAME", "group(B19013)"),
region = "county:*")
head(acs_group)
# Retreive block-level data within a specific tract using a nested regionin argument
data2010 <- getCensus(name = "dec/sf1",
vintage = 2010,
vars = c("NAME","P001001"),
region = "block:*",
regionin = "state:36+county:027+tract:010000")
head(data2010)
# Retreive block-level data for Decennial Census sf1, 2000
# Note, for this dataset a tract needs to be specified to retrieve blocks
data2000 <- getCensus(name = "sf1",
vintage = 2000,
vars = c("P001001", "P003001"),
region = "block:*",
regionin = "state:36+county:27+tract:010000")
listCensusMetadata
head(data2000)
# Get poverty rates for children and all ages over time
saipe <- getCensus(name = "timeseries/poverty/saipe",
vars = c("NAME", "SAEPOVRT0_17_PT", "SAEPOVRTALL_PT"),
region = "state:01",
time = "from 2000 to 2017")
head(saipe)
# Get county business patterns data for a specific NAICS sector
cbp_2016 <- getCensus(name = "cbp",
vintage = "2016",
vars = c("EMP", "ESTAB", "NAICS2012_TTL", "GEO_TTL"),
region = "state:*",
naics2012 = "23")
head(cbp_2016)
# Look at Relevant Data
getCensus(name = "pep/population",
vars = c("NAME", "IPRCAT", "IPR_DESC", "PCTUI_PT"),
region = "state:01",
time = 2017)
# Retreive block-level data within a specific tract using a nested regionin argument
data2010 <- getCensus(name = "dec/sf1",
vintage = 2010,
vars = c("NAME","P001001"),
region = "block:*",
regionin = "state:36+county:027+tract:010000")
head(data2010)
# Retreive block-level data within a specific tract using a nested regionin argument
data2010 <- getCensus(name = "dec/sf1",
vintage = 2010,
vars = c("NAME","P001001"),
region = "block:*",
regionin = "state:36+county:027+tract:010000")
head(data2010)
listCensusMetadata(name = "dec/sf1",
type = "geography")
listCensusMetadata(name = "dec/sf1",
type = "region")
listCensusMetadata(name = "dec/sf1",
type = "geography")
popest_housing <- getCensus(name = "pep/housing",
vintage = 2018,
vars = c("DATE_CODE", "DATE_DESC", "HUEST"),
region = "county:195",
regionin = "state:02")
head(popest_housing)
knitr::opts_chunk$set(echo = TRUE,
message = FALSE,
warning = FALSE)
# Load Libraries
library(dplyr)
library(readr)
library(randomForest)
library(rfUtilities)
library(rsample)
library(ggplot2)
library(caret)
library(e1071)
library(skimr)
#Loading and Discovery
# Load Data
data <- read_csv("insurance_claims.csv")
# First Look at Data
head(data)
# Check Types
sapply(data, class)
# Check for NAs
data[!complete.cases(data),] #non found
# Summarize Data
skim(data)
#Data Wrangling
data$fraud_reported <- as.integer(data$fraud_reported)
data$insured_zip <- as.character(data$insured_zip)
data$policy_bind_date <- as.Date(data$policy_bind_date, format = '%m/%d/%y')
# Check for amount of uniqe values
sapply(sapply(data, unique), length)
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !(names(data) %in% drops)]
# turn into factors
data <- data %>% mutate_if(is.character, as.factor)
# For reproducability
set.seed(52)
# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
# Basic Implementation & Analysis
# default RF model
rf1 <- randomForest(
formula = fraud_reported ~ .,
data    = data_train)
View(data_train)
#Loading and Discovery
# Load Data
data <- read_csv("insurance_claims.csv")
# First Look at Data
head(data)
# Check Types
sapply(data, class)
# Check for NAs
data[!complete.cases(data),] #non found
# Summarize Data
skim(data)
#Data Wrangling
data$insured_zip <- as.character(data$insured_zip)
data$policy_bind_date <- as.Date(data$policy_bind_date, format = '%m/%d/%y')
# Check for amount of uniqe values
sapply(sapply(data, unique), length)
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !(names(data) %in% drops)]
# turn into factors
data <- data %>% mutate_if(is.character, as.factor)
# For reproducability
set.seed(52)
# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
# Basic Implementation & Analysis
# default RF model
rf1 <- randomForest(
formula = fraud_reported ~ .,
data    = data_train)
# Model
rf1
# Plot tree
plot(rf1)
# Lowest MSE
which.min(rf1$mse)
# RMSE of this optimal random forest
# sqrt(rf1$mse[which.min(rf1$mse)])
#Variable Importance
rf1$importance
# Split down set further for validation set on training data
# create training and validation data
data_split <- initial_split(data_train, .8)
# training data
data_train_v2 <- analysis(data_split)
# validation data
data_valid <- assessment(data_split)
x_test <- data_valid[setdiff(names(data_valid),"fraud_reported")]
y_test <- data_valid$fraud_reported
rf_oob_comp <- randomForest(
formula = fraud_reported ~ .,
data    = data_train_v2,
xtest   = x_test,
ytest   = y_test)
# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
# names of features
features <- setdiff(names(data_valid),"fraud_reported")
# tune Random Forest
rf2 <- tuneRF(
x          = data_train[features],
y          = data_train$fraud_reported,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = TRUE      # to not show real-time progress
)
# Try best version
rf2 <- randomForest(
x          = data_train[features],
y          = data_train$fraud_reported,
data    = data_train,
ntree = 500,
mtry = 15
)
rf2
# Plot tree
plot(rf2)
#Variable Importance
rf2$importance
# names of features
features <- setdiff(names(data_valid),"fraud_reported")
# tune Random Forest
rf2 <- tuneRF(
x          = data_train[features],
y          = data_train$fraud_reported,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = TRUE      # to not show real-time progress
)
# Try best version
rf2 <- randomForest(
x          = data_train[features],
y          = data_train$fraud_reported,
data    = data_train,
ntree = 500,
mtry = 15
)
# Check result
rf2
rf2
# randomForest
pred_randomForest <- predict(rf2, data_test)
head(pred_randomForest)
# Make pred column
data_test$pred <- predict(rf2, data_test)
data_test$pred_final <- as.integer(ifelse(data_test$pred > 0.3 ,1 ,0))
# Confusion Matrix
confusionMatrix(as.factor(data_test$fraud_reported), as.factor(data_test$pred_final))
# randomForest
pred_randomForest <- predict(rf2, data_test)
# Plot tree
plot(rf2)
#Variable Importance
rf2$importance
# randomForest
pred_randomForest <- predict(rf2, data_test)
# randomForest
#pred_randomForest <- predict(rf2, data_test)
head(pred_randomForest)
knitr::opts_chunk$set(echo = TRUE,
message = FALSE,
warning = FALSE)
# Load Libraries
library(dplyr)
library(readr)
library(randomForest)
library(rfUtilities)
library(rsample)
library(ggplot2)
library(caret)
library(e1071)
library(skimr)
setwd("~/Documents/GitHub/RBusiness/Felix/Insurance/Insurance Fraud")
# Load Libraries
library(dplyr)
library(readr)
library(randomForest)
library(rfUtilities)
library(rsample)
library(ggplot2)
library(caret)
library(e1071)
library(skimr)
#Loading and Discovery
# Load Data
data <- read_csv("insurance_claims.csv")
# First Look at Data
head(data)
# Check Types
sapply(data, class)
# Check for NAs
data[!complete.cases(data),] #non found
# Summarize Data
skim(data)
#Data Wrangling
data$insured_zip <- as.character(data$insured_zip)
data$policy_bind_date <- as.Date(data$policy_bind_date, format = '%m/%d/%y')
# turn into factors
data <- data %>% mutate_if(is.character, as.factor)
# Check for amount of uniqe values
sapply(sapply(data, unique), length)
# Drop obvious colimns
drops <- c('incident_location','policy_bind_date','incident_date','insured_occupation','insured_zip', 'policy_number')
data <- data[ , !(names(data) %in% drops)]
# For reproducability
set.seed(52)
# Split Data
data_split <- initial_split(data, prop = .7)
data_train <- training(data_split)
data_test  <- testing(data_split )
# Basic Implementation & Analysis
# default RF model
rf1 <- randomForest(
formula = fraud_reported ~ .,
data    = data_train)
# Model
rf1
# Plot tree
plot(rf1)
#Variable Importance
rf1$importance
# names of features
features <- setdiff(names(data_valid),"fraud_reported")
# tune Random Forest
rf2 <- tuneRF(
x          = data_train[features],
y          = data_train$fraud_reported,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = TRUE      # to not show real-time progress
)
# Try best version
rf2 <- randomForest(
x          = data_train[features],
y          = data_train$fraud_reported,
data    = data_train,
ntree = 500,
mtry = 22
)
#Check results
rf2
# names of features
features <- setdiff(names(data_valid),"fraud_reported")
# tune Random Forest
rf2 <- tuneRF(
x          = data_train[features],
y          = data_train$fraud_reported,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = TRUE      # to not show real-time progress
)
# names of features
features <- setdiff(names(data_valid),"fraud_reported")
# tune Random Forest
rf2 <- tuneRF(
x          = data_train[features],
y          = data_train$fraud_reported,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = TRUE      # to not show real-time progress
)
# Try best version
rf2 <- randomForest(
x          = data_train[features],
y          = data_train$fraud_reported,
data    = data_train,
ntree = 500,
mtry = 22
)
#Check results
rf2
# randomForest
data_test$pred <- predict(rf2, data_test)
table(observed = data_test$fraud_reported, predicted = data_test$pred)
# create training and validation data
data_split <- initial_split(data_train, .8)
# validation data
data_valid <- assessment(data_split)
# names of features
features <- setdiff(names(data_valid),"fraud_reported")
# tune Random Forest
rf2 <- tuneRF(
x          = data_train[features],
y          = data_train$fraud_reported,
ntreeTry   = 500,
mtryStart  = 5,
stepFactor = 1.5,
improve    = 0.01,
trace      = TRUE      # to not show real-time progress
)
